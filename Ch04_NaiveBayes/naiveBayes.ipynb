{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(datasets):\n",
    "    \"\"\"\n",
    "    build a set including only unique tokens from all of sub-Text \n",
    "    \"\"\"\n",
    "    vocabSet = []\n",
    "    for data in datasets:\n",
    "        vocabSet.extend(data)\n",
    "    vocabSet = np.unique(vocabSet) \n",
    "    return vocabSet\n",
    "\n",
    "\n",
    "def word2Vec(vocabSet, inputSet):  # set-of-words model\n",
    "    vector = np.zeros(len(vocabSet))\n",
    "    for token in inputSet:\n",
    "        if token in vocabSet:\n",
    "            vector[list(vocabSet).index(token)] = 1 # set the value to 1 at the corresponds position\n",
    "    return vector\n",
    "\n",
    "def trainNB(trainMat, trainLabels):\n",
    "    \"\"\"\n",
    "    Compute the probability for each token in whole training set\n",
    "    that this token/word/feature exists in the text sets with certain\n",
    "    label 0(not abusive) or 1(abusive)\n",
    "    \n",
    "    Input: tokenized training texts and correponding class vectors\n",
    "    return: pAbusive--the prob. of abusive texts in the training sets\n",
    "            pNotAbusive--the prob. of non-abusive texts in the training sets\n",
    "            p0Vect--a vector contains the prob. for each token that this token\n",
    "                    exists in  non-abusive texts \n",
    "            p1Vect--a vector contains the prob. for each token that this token\n",
    "                    exists in abusive texts\n",
    "    \"\"\"\n",
    "    trainMat = np.array(trainMat)\n",
    "    trainLabels = np.array(trainLabels)\n",
    "    numTrainMat = len(trainMat)\n",
    "    pAbusive = len(trainLabels[trainLabels == 1]) / numTrainMat\n",
    "    pNotAbusive = 1 - pAbusive\n",
    "    nonAbusiveSets = trainMat[trainLabels == 0]  # sub-traingsets with class 0: non-abusive\n",
    "    abusiveSets = trainMat[trainLabels == 1]  # # sub-traingsets with class 1: abusive\n",
    "    # Compute the probability for each token in whole training set\n",
    "    # that this token/word/feature exists in the label=0 texts\n",
    "    p0Vect = (np.sum(nonAbusiveSets, axis=0) + 0.5) / (len(nonAbusiveSets) + 0.5) # add a tiny fraction 0.5 to avoid numerical problem\n",
    "    \n",
    "    # Compute the probability for each token in whole training set\n",
    "    # that this token/word/feature exists in the label=1 texts\n",
    "    p1Vect = (np.sum(abusiveSets, axis=0) + 0.5) / (len(abusiveSets) + 0.5)\n",
    "    return pNotAbusive, pAbusive, p0Vect, p1Vect\n",
    "\n",
    "\n",
    "def testNB(testsets, p0Vect, p1Vect, pNotAbusive, pAbusive):\n",
    "    testsets = np.array(testsets.reshape(-1, 1))\n",
    "    length = len(testsets)\n",
    "    # the prob. of being non-absuive text, using p0Vect\n",
    "    likelihoodVect0 = np.zeros((length, 1))\n",
    "    for i in range(length):\n",
    "        if testsets[i] == 1:\n",
    "            likelihoodVect0[i] = p0Vect[i]\n",
    "        else:\n",
    "            likelihoodVect0[i] = 1 - p0Vect[i]\n",
    "    likelihoodNotAbusive = np.prod(likelihoodVect0)  # production for each element: satisfy the iid. assumption for naive bayes\n",
    "    posteriorNotAbusive = likelihoodNotAbusive * pNotAbusive  # posterior prob. of sets \n",
    "    # the prob. of being absuive text, using p1Vect\n",
    "    likelihoodVect1 = np.zeros((length, 1))\n",
    "    for i in range(length):\n",
    "        if testsets[i] == 1:\n",
    "            likelihoodVect1[i] = p1Vect[i]\n",
    "        else:\n",
    "            likelihoodVect1[i] = 1 - p1Vect[i]\n",
    "    likelihoodAbusive = np.prod(likelihoodVect1)  # production for each element: satisfy the iid. assumption for naive bayes\n",
    "    posteriorAbusive = likelihoodAbusive * pAbusive  # posterior prob. of sets \n",
    "    \n",
    "    if posteriorNotAbusive > posteriorAbusive:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def bagOfword2Vec(vocabSet, inputSet):  # bag-of-words model\n",
    "    vector = np.zeros(len(vocabSet))\n",
    "    for token in inputSet:\n",
    "        if token in vocabSet:\n",
    "            vector[list(vocabSet).index(token)] += 1 # set the value to 1 at the corresponds position\n",
    "    return vector\n",
    "\n",
    "def generateTokens(file):\n",
    "    regEx = re.compile('\\\\W*') # filter any characters except words and numbers\n",
    "    fileName = listdir(file)\n",
    "    tokenSets = []\n",
    "    for name in fileName:\n",
    "        fr = open(file + name, encoding='gb18030', errors='ignore')\n",
    "        lists = fr.read()\n",
    "        tokens = regEx.split(lists.lower())\n",
    "        tokens = [token for token in tokens if len(token) > 2]  # drop the tokens with too short length\n",
    "        tokenSets.append(tokens)\n",
    "    return tokenSets\n",
    "\n",
    "def crossValidation(datasets, labels):\n",
    "    # split datasets into training sets and testing sets\n",
    "    n = len(datasets)\n",
    "    datasets = np.array(datasets)\n",
    "    accuracy = []\n",
    "    for i in range(int(n / 5)):\n",
    "        testIndex = np.random.permutation(n)[:int(n / 5)]  # pick n/5 sets at random as testing sets\n",
    "        trainIndex = list(set(np.arange(0, n)).difference(set(testIndex)))  # the rest as training sets\n",
    "        testSets = datasets[testIndex]\n",
    "        testLabels = labels[testIndex]\n",
    "        trainSets = datasets[trainIndex]\n",
    "        trainLabels = labels[trainIndex]\n",
    "        pHam, pSpam, p0Vect, p1Vect = trainNB(trainSets, trainLabels)\n",
    "        \n",
    "        errorCount = 0\n",
    "        for i in range(len(testSets)):\n",
    "            classLabel = testNB(testSets[i], p0Vect, p1Vect, pHam, pSpam)\n",
    "            if classLabel != testLabels[i]:\n",
    "                errorCount += 1\n",
    "        accuracy.append(1 - errorCount / len(testLabels))\n",
    "    accuracy = np.mean(accuracy)  # average value of 10 accuracy results\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataSet():\n",
    "    postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n",
    "                 ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n",
    "                 ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n",
    "                 ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n",
    "                 ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n",
    "                 ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n",
    "    classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not\n",
    "    return postingList,classVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['I', 'ate', 'buying', 'cute', 'dalmation', 'dog', 'flea', 'food',\n",
       "       'garbage', 'has', 'help', 'him', 'how', 'is', 'licks', 'love',\n",
       "       'maybe', 'mr', 'my', 'not', 'park', 'please', 'posting',\n",
       "       'problems', 'quit', 'so', 'steak', 'stop', 'stupid', 'take', 'to',\n",
       "       'worthless'], dtype='<U9')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfPosts, listOfClasses = loadDataSet()\n",
    "myVocabList = createVocabList(listOfPosts)\n",
    "myVocabList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = word2Vec(myVocabList, listOfPosts[0])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
       "        0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0.]),\n",
       " array([1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0.,\n",
       "        0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1.]),\n",
       " array([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,\n",
       "        1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0.]),\n",
       " array([0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1.])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainMat = [bagOfword2Vec(myVocabList, listOfPost) for listOfPost in listOfPosts]\n",
    "trainMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The prob. of non-abusive tests is:0.5\n",
      "The prob. of abusive tests is:0.5\n",
      "[0.42857143 0.42857143 0.14285714 0.42857143 0.42857143 0.42857143\n",
      " 0.42857143 0.14285714 0.14285714 0.42857143 0.42857143 0.71428571\n",
      " 0.42857143 0.42857143 0.42857143 0.42857143 0.14285714 0.42857143\n",
      " 1.         0.14285714 0.14285714 0.42857143 0.14285714 0.42857143\n",
      " 0.14285714 0.42857143 0.42857143 0.42857143 0.14285714 0.14285714\n",
      " 0.42857143 0.14285714]\n",
      "[0.14285714 0.14285714 0.42857143 0.14285714 0.14285714 0.71428571\n",
      " 0.14285714 0.42857143 0.42857143 0.14285714 0.14285714 0.42857143\n",
      " 0.14285714 0.14285714 0.14285714 0.14285714 0.42857143 0.14285714\n",
      " 0.14285714 0.42857143 0.42857143 0.14285714 0.42857143 0.14285714\n",
      " 0.42857143 0.14285714 0.14285714 0.42857143 1.         0.42857143\n",
      " 0.42857143 0.71428571]\n"
     ]
    }
   ],
   "source": [
    "pNotAbusive, pAbusive, p0Vect, p1Vect = trainNB(trainMat, listOfClasses)\n",
    "print(\"The prob. of non-abusive tests is:{}\".format(pNotAbusive))\n",
    "print(\"The prob. of abusive tests is:{}\".format(pAbusive))\n",
    "print(p0Vect)\n",
    "print(p1Vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "testText1 = ['love', 'my', 'dalmation']\n",
    "testText2 = ['stupid', 'garbage']\n",
    "testsets1 = word2Vec(myVocabList, testText1)\n",
    "testsets2 = word2Vec(myVocabList, testText2)\n",
    "class1 = testNB(testsets1, p0Vect, p1Vect, pNotAbusive, pAbusive)\n",
    "class2 = testNB(testsets2, p0Vect, p1Vect, pNotAbusive, pAbusive)\n",
    "print(class1)\n",
    "print(class2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtering spam e-mails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\1\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:91: FutureWarning: split() requires a non-empty pattern match.\n"
     ]
    }
   ],
   "source": [
    "regEx = re.compile('\\\\W*') # filter any characters except words and numbers\n",
    "\n",
    "hamFile = 'email/ham/'\n",
    "spamFile = 'email/spam/'\n",
    "hamTokenSets = generateTokens(hamFile)\n",
    "spamTokenSets = generateTokens(spamFile)\n",
    "emailLists = np.concatenate((hamTokenSets, spamTokenSets), axis=0)\n",
    "emailClasses = np.concatenate((np.zeros(len(hamTokenSets)), np.ones(len(spamTokenSets))), axis=0)  # two classes,ham:0,spam:1\n",
    "vocabSets = createVocabList(emailLists)\n",
    "emailDatasets = [word2Vec(vocabSets, emailList) for emailList in emailLists]  # number:50 dimension: 694"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is: 97.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = crossValidation(emailDatasets, emailClasses)\n",
    "print(\"The accuracy is: %.2f\" %(100 * accuracy) + '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
